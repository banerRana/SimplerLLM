from pydantic import BaseModel
from typing import Any, Optional, List, Dict
from datetime import datetime


class LLMFullResponse(BaseModel):
    """
    Full response object from LLM generation.

    This model contains the generated text along with metadata about the
    generation process, token usage, and provider-specific information.

    Attributes:
        generated_text: The text generated by the model.
        model: The model identifier used for generation.
        process_time: Time taken for the generation in seconds.
        input_token_count: Number of tokens in the input/prompt.
        output_token_count: Number of tokens in the generated output.
        llm_provider_response: Raw response object from the provider's API.
        model_object: Optional model instance used for generation.
        provider: The LLM provider enum value (e.g., LLMProvider.OPENAI).
        model_name: Human-readable model name.
        guardrails_metadata: Metadata from content validation/guardrails.
        web_sources: List of web sources used (for web search responses).
        reasoning_tokens: Number of tokens used for internal reasoning
            (only for reasoning models like o1, o3, GPT-5).
        finish_reason: Why the model stopped generating. Common values:
            "stop" (natural end), "length" (max tokens reached),
            "content_filter" (content was filtered).
        is_reasoning_model: Whether the response came from a reasoning-capable
            model (o1, o3, GPT-5 series).

    Example:
        >>> response = llm.generate_response(prompt="Hello", full_response=True)
        >>> print(f"Generated: {response.generated_text}")
        >>> print(f"Tokens: {response.input_token_count} in, {response.output_token_count} out")
        >>> if response.is_reasoning_model:
        ...     print(f"Reasoning tokens: {response.reasoning_tokens}")
    """
    generated_text: str
    model: str
    process_time: float
    input_token_count: Optional[int] = None
    output_token_count: Optional[int] = None
    llm_provider_response: Any
    model_object: Optional[Any] = None
    provider: Optional[Any] = None
    model_name: Optional[str] = None
    guardrails_metadata: Optional[Dict[str, Any]] = None
    web_sources: Optional[List[Dict[str, Any]]] = None

    # Reasoning model fields
    reasoning_tokens: Optional[int] = None
    """Number of tokens used for internal reasoning (OpenAI o1/o3/GPT-5, Gemini thinking models)."""

    thinking_content: Optional[str] = None
    """The model's reasoning/thinking process text (Gemini thinking models).

    For Gemini 2.5/3 thinking models, this contains the model's internal
    reasoning process that led to the final answer. Only populated when
    using thinking models with full_response=True.
    """

    finish_reason: Optional[str] = None
    """Why the model stopped generating (stop, length, content_filter, etc.)."""

    is_reasoning_model: bool = False
    """Whether this response came from a reasoning-capable model."""

    extraction_result: Optional[Any] = None
    """Pattern extraction result when using generate_structured_pattern with full_response=True."""


class LLMEmbeddingsResponse(BaseModel):
    generated_embedding: Any
    model: str
    process_time: float
    llm_provider_response: Any


class PatternMatch(BaseModel):
    """Represents a single pattern match extracted from text."""
    value: str
    """The extracted value as found in the text"""

    normalized_value: Optional[str] = None
    """The normalized version of the value (if normalization was applied)"""

    pattern_type: str
    """The type of pattern matched (e.g., 'email', 'phone', 'custom')"""

    position: int
    """The starting position of the match in the original text"""

    is_valid: bool = True
    """Whether the match passed validation checks beyond regex"""

    validation_message: Optional[str] = None
    """Validation details or error message if validation failed"""

    confidence: Optional[float] = None
    """Match quality score (0-1), if applicable"""

    class Config:
        json_schema_extra = {
            "example": {
                "value": "john.doe@example.com",
                "normalized_value": "john.doe@example.com",
                "pattern_type": "email",
                "position": 42,
                "is_valid": True,
                "validation_message": "Valid email format",
                "confidence": 1.0
            }
        }


class PatternExtractionResult(BaseModel):
    """Result of a pattern extraction operation from LLM output."""
    matches: List[PatternMatch]
    """List of all extracted pattern matches"""

    total_matches: int
    """Total number of matches found"""

    pattern_used: str
    """The regex pattern that was used for extraction"""

    original_text: str
    """The original text from the LLM response"""

    extraction_timestamp: datetime
    """When the extraction was performed"""

    class Config:
        json_schema_extra = {
            "example": {
                "matches": [
                    {
                        "value": "john@example.com",
                        "normalized_value": "john@example.com",
                        "pattern_type": "email",
                        "position": 0,
                        "is_valid": True,
                        "validation_message": "Valid email format",
                        "confidence": 1.0
                    }
                ],
                "total_matches": 1,
                "pattern_used": r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                "original_text": "Contact us at john@example.com for more information.",
                "extraction_timestamp": "2025-01-15T10:30:00"
            }
        }
